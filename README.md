# ðŸ¥‡ Quantitative Gold Analysis: A Scientific Approach to Market Sentiment & LSTMs

> **NLP Sentiment Analysis Ã— LSTM Time-Series Modeling**  
> Talento-Tech Bootcamp 2025-2 Â· Universidad de Antioquia Â· MedellÃ­n, Colombia

[![Python 3.8+](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status: Post-Mortem](https://img.shields.io/badge/Status-Post--Mortem%20Audited-orange.svg)](#5-the-integrators-audit-known-technical-debt)

### ðŸ‘‰ [VersiÃ³n en EspaÃ±ol aquÃ­](README.es.md)

---

## Table of Contents

1. [Project Overview](#1-project-overview)
2. [The Team: A Physics-Led Squad](#2-the-team-a-physics-led-squad)
3. [The "Federation" Architecture](#3-the-federation-architecture)
4. [Quick Start](#4-quick-start)
5. [The Integrator's Audit: Known Technical Debt](#5-the-integrators-audit-known-technical-debt)
6. [Lessons Learned](#6-lessons-learned)
7. [Future Roadmap: The Lakehouse Evolution](#7-future-roadmap-the-lakehouse-evolution)
8. [Project Structure](#8-project-structure)

---

## 1. Project Overview

Can the sentiment of financial news **predict** gold price movements?

This project builds an end-to-end pipeline that:
- **Scrapes** ~189,000 Wall Street Journal headlines (2016â€“2025).
- **Filters** ~18,700 gold-related articles using keyword heuristics.
- **Scores** each headline with **FinBERT** (ProsusAI/finbert), a transformer model fine-tuned for financial sentiment.
- **Detects anomalies** in gold price data using statistical methods.
- **Tests causality** between sentiment and price via Granger Causality.
- **Predicts** daily gold closing prices with LSTM networks â€” comparing a base model (price-only features) against a sentiment-enhanced model.

**Key finding:** Integrating FinBERT sentiment features into LSTM improved prediction accuracy, though the causal signal is nuanced. The full analysis, including statistical caveats, is documented across 8 notebooks.

---

## 2. The Team: A Physics-Led Squad

### Core Technical Governance

| Role | Member | Responsibility |
|------|--------|---------------|
| **Technical Lead & Integrator** | Pablo Sanchez *(Physics â€“ UdeA)* | UnificaciÃ³n pipeline, 8-notebook architecture, Anomaly Detection, post-project audit |
| **Co-Lead & Statistical Analyst** | Jose Ortiz *(Physics â€“ UdeA)* | Web scraping infrastructure, Granger Causality analysis, data validation |

Pablo and Jose formed the project's **governance pair**. Pablo guaranteed the pipeline ran end-to-end; Jose guaranteed the statistical claims held up under scrutiny. This dual-stewardship model â€” analogous to a Platform Engineer and a Quant Analyst â€” ensured that data integrity was protected across the full "front-to-back" engineering flow.

### The Original Squad

| Member | Background | Contribution |
|--------|------------|-------------|
| David Alava | Physics â€“ UdeA | NLP Specialist (FinBERT implementation) |
| Sebastian Agudelo | Physics â€“ UdeA | NLP Specialist (FinBERT implementation) |
| Dayana Henao | Physics â€“ UdeA | ML Engineer (LSTM & Gold Price EDA) |
| Luis Vera | Forestry Engineering | EDA Specialist (News data analysis) |
| Michael Tarazona | Electrical Engineering â€“ UdeA | Junior support |

> **A note on inclusive leadership:** Roles were assigned based on capability, not credential. Luis (Forestry Engineering) handled News EDA because his analytical rigor was excellent â€” not because of his degree title.

---

## 3. The "Federation" Architecture

We did **not** build a monolith. We built a **federation of scientific deliverables**.

Each notebook represents a domain boundary owned by a specific team member, with explicit CSV/JSON input-output contracts. This was a deliberate integration strategy to let 7 independent contributors work in parallel without blocking each other.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE INTEGRATION PIPELINE                     â”‚
â”‚                                                                 â”‚
â”‚  ðŸ“¥ INGESTION        ðŸ“Š EXPLORATION       ðŸ”¬ ANALYSIS          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ 01       â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ 02       â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ 04           â”‚      â”‚
â”‚  â”‚ Data Loadâ”‚        â”‚ Price EDAâ”‚        â”‚ Anomaly Det. â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚       â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚              â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ 03       â”‚               â”‚              â”‚
â”‚                      â”‚ News EDA â”‚               â”‚              â”‚
â”‚                      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚              â”‚
â”‚                           â”‚                     â”‚              â”‚
â”‚  ðŸ§  NLP                  â”‚         ðŸ“ˆ MODELING  â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ 05           â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ 06                 â”‚      â”‚
â”‚  â”‚ FinBERT      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Correlation &      â”‚      â”‚
â”‚  â”‚ Sentiment    â”‚                 â”‚ Granger Causality  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                            â”‚                   â”‚
â”‚  ðŸ¤– PREDICTION           ðŸ“‹ SYNTHESIS      â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                   â”‚
â”‚  â”‚ 07           â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”¤ Integrated  â”‚â—€â”€â”€â”˜                   â”‚
â”‚  â”‚ LSTM Models  â”‚        â”‚ Dataset     â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚         â”‚                                                      â”‚
â”‚         â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  â”‚ 08           â”‚                                              â”‚
â”‚  â”‚ Synthesis &  â”‚                                              â”‚
â”‚  â”‚ Results      â”‚                                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| # | Notebook | Owner(s) | Purpose |
|---|----------|----------|---------|
| 01 | IntroducciÃ³n y Carga de Datos | Pablo | Load hourly bars + news; resample to daily; validate alignment |
| 02 | EDA Precios del Oro | Dayana | Statistical analysis, stationarity tests, seasonal decomposition |
| 03 | EDA Noticias WSJ | Luis | News volume, temporal coverage, keyword filtering |
| 04 | DetecciÃ³n de AnomalÃ­as | Pablo | Outlier detection in gold prices via statistical methods |
| 05 | AnÃ¡lisis de Sentimientos (FinBERT) | David & Sebastian | FinBERT inference on ~18K headlines; daily aggregation |
| 06 | CorrelaciÃ³n y Causalidad | Jose | Pearson/Spearman correlation, Granger Causality tests |
| 07 | Modelo LSTM Integrado | Dayana & Pablo | Base vs. sentiment-enhanced LSTM comparison |
| 08 | SÃ­ntesis y Resultados | Pablo | Final report generation, cross-notebook synthesis |

---

## 4. Quick Start

### Prerequisites
- Python 3.8+
- ~4 GB of disk space (FinBERT model cache)

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/your-username/gold-prediction-pipeline.git
cd gold-prediction-pipeline

# 2. Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Edit .env if your data is not in the project root
# Default: BASE_DIR=.
```

### Running the Notebooks

```bash
cd unificacion/notebooks
jupyter notebook
```

Execute notebooks **in numerical order** (01 â†’ 08). Each notebook reads outputs from previous ones.

> **âš ï¸ Portability Note:** This project originally used hardcoded absolute paths. These have been replaced with environment-variable-driven configuration via `python-dotenv`. If you encounter path issues, verify your `.env` file.

### ðŸ“Š Dataset Availability

To keep this repository lightweight, the data files included here are samples (first 500 rows). This allows you to run the notebooks and verify the pipeline logic immediately.

**Full Dataset:** If you wish to reproduce the complete study with all ~189,000 headlines and full price history (~160MB), you can download the complete database here: [https://drive.google.com/drive/folders/1osPy3E6g6bIYcpd54menGyOlnp2SlJog?usp=sharing]

---

## 5. The Integrator's Audit: Known Technical Debt

> *"The mark of a senior engineer is not writing perfect code â€” it's knowing exactly where your code is imperfect and why."*

After the bootcamp concluded, I conducted a post-project architectural audit. Below are the findings I documented â€” not to hide our team's work, but to demonstrate that I understand the gap between a bootcamp prototype and production-grade engineering.

### ðŸ”´ Look-ahead Bias (Data Leakage)

**What:** Sentiment moving averages in Notebook 05 use `rolling(window=7, center=True)`. The `center=True` parameter means the feature for day *t* incorporates sentiment from days *t+1, t+2, t+3* â€” future data that wouldn't exist at prediction time.

**Impact:** Model performance metrics (RMSE, MAE) may be artificially optimistic because the LSTM had indirect access to future sentiment signals.

**Fix:** Replace centered windows with strictly causal (trailing) windows: `rolling(window=7, center=False)`.

### ðŸŸ¡ Time-Frequency Alignment

**What:** News timestamps are collapsed to calendar date (`dt.date`) before merging with daily price bars. This ignores:
- After-market-close news being assigned to the same day's closing price.
- Timezone mismatches between UTC price data and local-time news timestamps.

**Impact:** Potential contamination of same-day features with information that wasn't available during the trading session.

**Fix:** Normalize all timestamps to UTC; use as-of joins with market calendar awareness.

### ðŸŸ¡ Silent Data Loss via Inner Joins

**What:** The integration step in Notebook 06 uses `df_precios.join(df_sentimientos, how='inner')`, which silently drops any date that doesn't appear in both DataFrames.

**Impact:** Trading days without news coverage are excluded from analysis and modeling, potentially biasing the dataset toward "eventful" days.

**Fix:** Use a left join on the price axis and explicitly handle missing sentiment (e.g., forward-fill or neutral imputation).

### ðŸŸ¢ Portability (Resolved)

**What:** All file paths originally referenced `/home/els4nchez/Videos/TECH/...`.

**Status:** âœ… Fixed. Paths now use `os.getenv('BASE_DIR')` via `python-dotenv`.

---

## 6. Lessons Learned

- **Evolution from Scripting to Engineering:** We successfully built a functional LSTM pipeline using Jupyter Notebooks for rapid prototyping. I learned that this monolithic-per-notebook structure hinders modularity. Future iterations would refactor data processing logic into a standalone Python package to enable unit testing and CI/CD integration.

- **Infrastructure Agnosticism:** The project originally relied on local file paths. A key takeaway was the necessity of environment-variable-driven configuration (12-Factor App principles) to ensure the pipeline runs identically on a developer's laptop, a CI runner, or a cloud container.

- **Separation of Concerns in ML:** We tightly coupled feature engineering with model training. I recognized that separating these into distinct steps (e.g., using a tool like Apache Airflow or Prefect) would allow for better error handling, reproducibility, and incremental data processing without retraining the entire model.

---

## 7. Future Roadmap: The Lakehouse Evolution

If this project were to evolve toward a production-grade system (e.g., at Bancolombia), the architecture would follow a **Lakehouse / Bronze-Silver-Gold** pattern:

| Layer | Purpose | Current State | Production Target |
|-------|---------|---------------|-------------------|
| **ðŸ¥‰ Bronze** | Raw, immutable ingestion | CSV files in `data/raw/` | Parquet/Delta partitioned by date in cloud object storage |
| **ðŸ¥ˆ Silver** | Cleaned, normalized, validated | `datos_procesados/*.csv` | Schema-validated, UTC-normalized, with data quality gates |
| **ðŸ¥‡ Gold** | Business-ready features | `datos_integrados_*.csv` | Feature store with point-in-time correctness and lineage tracking |

### Key Production Components

- **Feature Store** (e.g., Feast): Enforce that every feature for time *t* is computed using only data available at or before *t*. This eliminates look-ahead bias by design.
- **Pipeline Orchestration** (e.g., Airflow/Prefect): Replace manual notebook execution with versioned, testable DAGs.
- **Experiment Tracking** (e.g., MLflow): Replace `print()` statements with structured metric logging.
- **Containerization** (Docker + CI/CD): Ensure reproducibility across environments.

---

## 8. Project Structure

```
â”œâ”€â”€ .env.example                          # Env template (BASE_DIR=. default)
â”œâ”€â”€ .github/                              # Repo metadata (workflows, context)
â”œâ”€â”€ README.md                             # English guide
â”œâ”€â”€ README.es.md                          # Spanish guide
â”œâ”€â”€ requirements.txt                      # Root dependencies
â”œâ”€â”€ filtrado_noticias.py                  # WSJ headline filtering script
â”œâ”€â”€ data/                                 # Local data placeholder (contents local/manual)
â”‚   â”œâ”€â”€ raw/                              # Raw scraped data + samples (local)
â”‚   â””â”€â”€ processed/                        # Filtered articles + samples (local)
â”œâ”€â”€ datos_horas/                          # Hourly gold price bars (local)
â””â”€â”€ unificacion/
    â”œâ”€â”€ requirements.txt                  # Pipeline dependencies
    â”œâ”€â”€ notebooks/
    â”‚   â”œâ”€â”€ 01_Introduccion_y_Carga_de_Datos.ipynb
    â”‚   â”œâ”€â”€ 02_EDA_Precios_Oro.ipynb
    â”‚   â”œâ”€â”€ 03_EDA_Noticias_WSJ.ipynb
    â”‚   â”œâ”€â”€ 04_Deteccion_Anomalias.ipynb
    â”‚   â”œâ”€â”€ 05_Analisis_Sentimientos_FinBERT.ipynb
    â”‚   â”œâ”€â”€ 06_Correlacion_y_Causalidad.ipynb
    â”‚   â”œâ”€â”€ 07_Modelo_LSTM_Integrado.ipynb
    â”‚   â””â”€â”€ 08_Sintesis_y_Resultados.ipynb
    â”œâ”€â”€ datos_procesados/                 # Processed outputs (local/generated)
    â”œâ”€â”€ modelos/                          # Trained models (.keras) (local)
    â”œâ”€â”€ figuras/                          # Plotly figures (local)
    â””â”€â”€ informes/                         # Summary tables (local)
```

---

## Acknowledgments

This project was developed as part of the **Talento-Tech Bootcamp (2025-2)** in collaboration with the **Universidad de Antioquia**, MedellÃ­n. Special thanks to the bootcamp instructors for creating the environment that made this scientific collaboration possible.

The post-project architectural audit was conducted independently as preparation for the **Bancolombia Talento B** program, applying enterprise engineering standards to a bootcamp prototype.

---

<p align="center">
  <i>Built by physicists. Integrated by an engineer-in-training. Audited with honesty.</i>
</p>
